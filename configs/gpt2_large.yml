# DISCLAIMER: This is the configuration file for the GPT-NeoX-20B model as it was trained on 96x 40GB A100
# GPUs. Depending on your system configuration, you may need to change some parameters in order to fit
# the model in memory.

{

  "world_size": 8,
  "global_num_gpus": 8,
  "n_embd": 1280,    
  "n_ctx": 1024,
  "layer_norm_epsilon": 0.00001,
  "attn_pdrop": 0.1,
  "resid_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "rotary_dim": 64,
  "activation_function": 'gelu_new',
  "scaled_upper_triang_masked_softmax_fusion": false,
  "make_vocab_size_divisible_by": 1,
  "finetune": true,
  # "precision": "bfloat16",
  # "precision": "fp32"
  "seed": 1234,
  "icl_or_neo": "icl",
  # "data-path": "/nas2/lishengping/caiyun_projects/MetaICL/tensorized/filter2000/eval",
  # "data-path": "/nas2/lishengping/caiyun_projects/MetaICL/tensorized/filter2000/",
  # "data_path": "/nas2/lishengping/caiyun_projects/MetaICL/tensorized/unseen_len768/eval",
  # "data_path": "/nas2/lishengping/caiyun_projects/MetaICL/tensorized/small",
  "data_path": "/nas2/lishengping/caiyun_projects/MetaICL/tensorized/meta_train",
  
  "only_eval": false,
  # Tokenizer /  checkpoint settings - you will need to change these to the location you have them saved in

  # "load": "./6B-tensor-para-exp_checkpoints-mp8-pp1",

  # If finetuning, edit the following to the location of your finetuning dataset:
  # "data-path": "./data/pile_20B_tokenizer/pile_20B_tokenizer_text_document",

  # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages
  # across the node boundaries )
  "pipe-parallel-size": 8,
  "model-parallel-size": 1,

  # model settings
  "num-layers": 36,
  "hidden-size": 1280,
  "num-attention-heads": 20,
  "seq-length": 768,
  "max-position-embeddings": 1024,
  "norm": "layernorm",
  "pos-emb": "learned", # @lsp
  "rotary_pct": 0.25,
  "no-weight-tying": false,
  "gpt_j_residual": true,
  "output_layer_parallelism": "column",
  "scaled-upper-triang-masked-softmax-fusion": true,
  "bias-gelu-fusion": true,
  # init methods
  "init_method": "small_init",
  "output_layer_init_method": "wang_init",

  # optimizer settings
  "optimizer": {
    "type": "adam",
    "max_grad_norm": 1.0, 
    "params": {
      "lr": 0.000001,
      "betas": [0.9, 0.995],
      "eps": 1.0e-8,
      }
      },

  "min_lr": 0.0000001,
  "zero_optimization": {
  "stage": 1,
  "allgather_partitions": True,
  "allgather_bucket_size": 500000000,
  "overlap_comm": True,
  "reduce_scatter": True,
  "reduce_bucket_size": 500000000,
  "contiguous_gradients": True,
  "cpu_offload": False
  },

  # batch / data settings (assuming 96 GPUs)
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "data-impl": "mmap",
  "split": "995,4,1",

  # activation checkpointing
  "checkpoint-activations": false,
  "checkpoint-num-layers": 1,
  "partition-activations": false,
  "synchronize-each-layer": true,

  # regularization
  "gradient_clipping": 1.0,
  "weight-decay": 0, # @lsp
  "hidden-dropout": 0,
  "attention-dropout": 0,

  # precision settings
  "fp16": {
    "fp16": true,
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 0, #lsp 1 -> 0, 初始loss_scale 2 ** 0
    "hysteresis": 2,
    "min_loss_scale": 1
    },

  # misc. training settings
  "train-iters": 40001,
  "lr-decay-iters": 40001,

  "distributed-backend": "nccl",
  "lr-decay-style": "linear",
  "warmup": 0.0,
  "save-interval": 5000,
  "eval-interval": 500,
  "eval-iters": 100,

  # logging
  "log-interval": 10,
  "steps_per_print": 10,
  "wall_clock_breakdown": false,

  ### NEW DATA: ####
  #"tokenizer_type": "HFTokenizer",
  #"tensorboard-dir": "./tensorboard",
  #"log-dir": "./logs",
}