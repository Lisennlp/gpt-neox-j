{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b435e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1eb37b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'comm' from 'deepspeed' (/nas2/kf/miniconda3/envs/pytorch1.10/lib/python3.8/site-packages/deepspeed/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m comm \u001b[38;5;28;01mas\u001b[39;00m dist\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'comm' from 'deepspeed' (/nas2/kf/miniconda3/envs/pytorch1.10/lib/python3.8/site-packages/deepspeed/__init__.py)"
     ]
    }
   ],
   "source": [
    "# from deepspeed import comm as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe15d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.pipe import PipelineModule, LayerSpec, TiedLayerSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c1ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da2d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "942f08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/nas/shawn_guo/gpt_neox/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7dbb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_model_gptj import GPT2ModelPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d85f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8cee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed import init_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f687b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_distributed in module deepspeed.utils.distributed:\n",
      "\n",
      "init_distributed(dist_backend='nccl', auto_mpi_discovery=True, distributed_port=29500, verbose=True, timeout=datetime.timedelta(seconds=1800), init_method=None)\n",
      "    Initialize torch.distributed backend, potentially performing MPI discovery if needed\n",
      "    \n",
      "    Arguments:\n",
      "        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo\n",
      "    \n",
      "        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n",
      "    \n",
      "        distributed_port: Optional (int). torch distributed backend port\n",
      "    \n",
      "        verbose: Optional (bool). verbose logging\n",
      "    \n",
      "        timeout: Optional (timedelta). Timeout for operations executed against the process group. Default value equals 30 minutes.\n",
      "    \n",
      "        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(init_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd85c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_ADDR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m192.168.53.6\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8920\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = '192.168.53.6'\n",
    "os.environ[\"MASTER_PORT\"] = '8920'\n",
    "os.environ[\"WORLD_SIZE\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepspeed.comm.comm import init_distributed\n",
    "# init_distributed(dist_backend=\"nccl\",\n",
    "#                      auto_mpi_discovery=True,\n",
    "#                     distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT,\n",
    "#                     verbose=True,\n",
    "#                     timeout=default_pg_timeout,\n",
    "#                      init_method=None,\n",
    "#                     dist_init_required=None,\n",
    "#                      config=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c6a12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "789e7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTJArgs:\n",
    "    hidden_size = 4096\n",
    "    checkpoint_num_layers = 1\n",
    "    num_layers = 28\n",
    "    attention_config = ['global']*28\n",
    "    checkpoint_activations = False\n",
    "    pipe_partition_method = 'parameters'  # default\n",
    "    no_weight_tying = False\n",
    "    padded_vocab_size = 50400\n",
    "    max_position_embeddings = 2048\n",
    "    hidden_dropout = 0.0 \n",
    "    pos_emb = \"rotary\" # gxh\n",
    "    num_attention_heads = 16\n",
    "    fp16_lm_cross_entropy = False # gxh\n",
    "    is_pipe_parallel = True\n",
    "    init_method = \"normal\"\n",
    "    output_layer_init_method = \"normal\"\n",
    "    init_method_std = \"normal\"\n",
    "    norm = \"layernorm\"\n",
    "    layernorm_epsilon = 1e-4\n",
    "    deepspeed = True\n",
    "    use_bnb_optimizer = False\n",
    "    opt_pos_emb_offset = 0\n",
    "    n_embd = 4096\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    n_inner = None\n",
    "    attn_pdrop = 0.0\n",
    "    attn_pdrop = 0.1\n",
    "    resid_pdrop = 0.0\n",
    "    rotary_dim = 64\n",
    "    activation_function = 'gelu_new'\n",
    "    num_stages = 8\n",
    "    scaled_upper_triang_masked_softmax_fusion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be423136",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptj_args = GPTJArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e740404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas2/kf/miniconda3/envs/pytorch1.10/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "fatal: detected dubious ownership in repository at '/nas/shawn_guo/gpt_neox'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /nas/shawn_guo/gpt_neox\n"
     ]
    }
   ],
   "source": [
    "from megatron.utils import print_rank_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38b570a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(gptj_args, use_cache=False):\n",
    "    \"\"\"Build the model.\"\"\"\n",
    "\n",
    "    print_rank_0(\"building GPT2 model ...\")\n",
    "\n",
    "    # Build model on cpu.\n",
    "    model = GPT2ModelPipe(\n",
    "        gptj_args=gptj_args,\n",
    "        num_tokentypes=0,\n",
    "        parallel_output=True,\n",
    "        topology=X,\n",
    "        use_cache=use_cache,\n",
    "    )\n",
    "\n",
    "    if gptj_args.is_pipe_parallel:\n",
    "        # Export PipeParallel model to nn.Sequential model to avoid the overhead of deepspeed's pipe parallel training\n",
    "        model = model.to_sequential()\n",
    "\n",
    "    if gptj_args.deepspeed:\n",
    "        # DeepSpeed handles CUDA, FP16, and DDP components.\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(\"Must be using deepspeed to run neox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ee2adb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building GPT2 model ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GPT2ModelPipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgptj_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [19], line 7\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(gptj_args, use_cache)\u001b[0m\n\u001b[1;32m      4\u001b[0m print_rank_0(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding GPT2 model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Build model on cpu.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2ModelPipe\u001b[49m(\n\u001b[1;32m      8\u001b[0m     gptj_args\u001b[38;5;241m=\u001b[39mgptj_args,\n\u001b[1;32m      9\u001b[0m     num_tokentypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     10\u001b[0m     parallel_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     topology\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m     12\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gptj_args\u001b[38;5;241m.\u001b[39mis_pipe_parallel:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Export PipeParallel model to nn.Sequential model to avoid the overhead of deepspeed's pipe parallel training\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_sequential()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2ModelPipe' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model(gptj_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d2a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dd18b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed import comm as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c69c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_distributed in module deepspeed.comm.comm:\n",
      "\n",
      "init_distributed(dist_backend='nccl', auto_mpi_discovery=True, distributed_port=29500, verbose=True, timeout=datetime.timedelta(seconds=1800), init_method=None, dist_init_required=None, config=None)\n",
      "    Initialize dist backend, potentially performing MPI discovery if needed\n",
      "    \n",
      "    Arguments:\n",
      "        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo\n",
      "        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n",
      "        distributed_port: Optional (int). torch distributed backend port\n",
      "        verbose: Optional (bool). verbose logging\n",
      "        timeout: Optional (timedelta). Timeout for operations executed against the process group. Default value equals 30 minutes.\n",
      "        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified.\n",
      "        config: Optional (dict). DeepSpeed configuration for setting up comms options (e.g. Comms profiling)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deepspeed.init_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760a326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_distributed in module deepspeed.comm.comm:\n",
      "\n",
      "init_distributed(dist_backend='nccl', auto_mpi_discovery=True, distributed_port=29500, verbose=True, timeout=datetime.timedelta(seconds=1800), init_method=None, dist_init_required=None, config=None)\n",
      "    Initialize dist backend, potentially performing MPI discovery if needed\n",
      "    \n",
      "    Arguments:\n",
      "        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo\n",
      "        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n",
      "        distributed_port: Optional (int). torch distributed backend port\n",
      "        verbose: Optional (bool). verbose logging\n",
      "        timeout: Optional (timedelta). Timeout for operations executed against the process group. Default value equals 30 minutes.\n",
      "        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified.\n",
      "        config: Optional (dict). DeepSpeed configuration for setting up comms options (e.g. Comms profiling)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deepspeed.init_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ded4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c924e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_distributed in module deepspeed.comm.comm:\n",
      "\n",
      "init_distributed(dist_backend='nccl', auto_mpi_discovery=True, distributed_port=29500, verbose=True, timeout=datetime.timedelta(seconds=1800), init_method=None, dist_init_required=None, config=None)\n",
      "    Initialize dist backend, potentially performing MPI discovery if needed\n",
      "    \n",
      "    Arguments:\n",
      "        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo\n",
      "        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n",
      "        distributed_port: Optional (int). torch distributed backend port\n",
      "        verbose: Optional (bool). verbose logging\n",
      "        timeout: Optional (timedelta). Timeout for operations executed against the process group. Default value equals 30 minutes.\n",
      "        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified.\n",
      "        config: Optional (dict). DeepSpeed configuration for setting up comms options (e.g. Comms profiling)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deepspeed.init_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e49564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d076db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12345'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3280bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-09 16:35:58,837] [WARNING] [comm.py:160:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n"
     ]
    }
   ],
   "source": [
    "dist.init_deepspeed_backend(ds_backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73d420eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-09 16:37:54,222] [INFO] [comm.py:635:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-92a4572501bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_backend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nccl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauto_mpi_discovery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'env://'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/comm/comm.py\u001b[0m in \u001b[0;36minit_distributed\u001b[0;34m(dist_backend, auto_mpi_discovery, distributed_port, verbose, timeout, init_method, dist_init_required, config)\u001b[0m\n\u001b[1;32m    635\u001b[0m                         dist_backend))\n\u001b[1;32m    636\u001b[0m             \u001b[0;31m# Create a torch backend object, initialize torch distributed, and assign to cdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mcdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/comm/torch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, timeout, init_method, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# it is not so we can run on a single GPU without doing any init_process_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/comm/torch.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(self, backend, timeout, init_method)\u001b[0m\n\u001b[1;32m     34\u001b[0m             torch.distributed.init_process_group(backend,\n\u001b[1;32m     35\u001b[0m                                                  \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                                                  init_method=init_method)\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mpi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0minit_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             )\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendezvous_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/torch/distributed/rendezvous.py\u001b[0m in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_env_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RANK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"world_size\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/torch/distributed/rendezvous.py\u001b[0m in \u001b[0;36m_get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0menv_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_env_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "dist.init_distributed(dist_backend='nccl',auto_mpi_discovery=False,init_method='env://')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80e753f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\", world_size=torch.cuda.device_count(),\n",
    "                                              rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56134a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_distributed in module deepspeed.comm.comm:\n",
      "\n",
      "init_distributed(dist_backend='nccl', auto_mpi_discovery=True, distributed_port=29500, verbose=True, timeout=datetime.timedelta(seconds=1800), init_method=None, dist_init_required=None, config=None)\n",
      "    Initialize dist backend, potentially performing MPI discovery if needed\n",
      "    \n",
      "    Arguments:\n",
      "        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo\n",
      "        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n",
      "        distributed_port: Optional (int). torch distributed backend port\n",
      "        verbose: Optional (bool). verbose logging\n",
      "        timeout: Optional (timedelta). Timeout for operations executed against the process group. Default value equals 30 minutes.\n",
      "        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified.\n",
      "        config: Optional (dict). DeepSpeed configuration for setting up comms options (e.g. Comms profiling)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dist.init_distributed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c650c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialWrapper(\n",
       "  (sequential): Sequential(\n",
       "    (0): EmbeddingPipe(\n",
       "      (word_embeddings): Embedding(50400, 4096)\n",
       "      (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): Lambda()\n",
       "    (2): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (24): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (25): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (26): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (27): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (28): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (29): TransformerLayerPipe(\n",
       "      (ln_1): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (30): Lambda()\n",
       "    (31): NormPipe(\n",
       "      (norm): LayerNorm((4096,), eps=0.0001, elementwise_affine=True)\n",
       "    )\n",
       "    (32): Lambda()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "75b68619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59e31a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e39db29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.runtime.pipe.topology import PipelineParallelGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e901ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed import comm as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a479573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f798d476",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "open(/distributed_test): Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b19d5021ceba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                         \u001b[0minit_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"file:///distributed_test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         rank=0)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# Use store based barrier here since barrier() used a bunch of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;31m# default devices and messes up NCCL internal state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0m_store_based_barrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;31m# Set sequence numbers for gloo and nccl process groups.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_pg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNCCL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36m_store_based_barrier\u001b[0;34m(rank, store, timeout)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \"\"\"\n\u001b[1;32m    226\u001b[0m     \u001b[0mstore_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTORE_BASED_BARRIER_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_group_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Added key: {} to store for rank: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: open(/distributed_test): Permission denied"
     ]
    }
   ],
   "source": [
    "torch.distributed.init_process_group(backend=\"nccl\",\n",
    "                        init_method=\"file:///distributed_test\",\n",
    "                        world_size=2,\n",
    "                        rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "553f2e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-09 15:28:55,553] [WARNING] [comm.py:160:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n"
     ]
    }
   ],
   "source": [
    "dist.init_deepspeed_backend(ds_backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e51912ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_distributed in module deepspeed.comm.comm:\n",
      "\n",
      "init_distributed(dist_backend='nccl', auto_mpi_discovery=True, distributed_port=29500, verbose=True, timeout=datetime.timedelta(seconds=1800), init_method=None, dist_init_required=None, config=None)\n",
      "    Initialize dist backend, potentially performing MPI discovery if needed\n",
      "    \n",
      "    Arguments:\n",
      "        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo\n",
      "        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n",
      "        distributed_port: Optional (int). torch distributed backend port\n",
      "        verbose: Optional (bool). verbose logging\n",
      "        timeout: Optional (timedelta). Timeout for operations executed against the process group. Default value equals 30 minutes.\n",
      "        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified.\n",
      "        config: Optional (dict). DeepSpeed configuration for setting up comms options (e.g. Comms profiling)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dist.init_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73cf0d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-08 12:55:50,832] [INFO] [comm.py:618:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2022-09-08 12:55:50,950] [INFO] [comm.py:675:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.53.6, master_port=29500\n",
      "[2022-09-08 12:55:50,952] [INFO] [comm.py:635:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "if dist.init_distributed():\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7283a1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3ca796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.runtime.pipe import ProcessTopology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc4647ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "362790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "num_stages (2) must divide distributed world size (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-88b11ed4b235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_stages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/runtime/pipe/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, num_stages, topology, loss_fn, seed_layers, seed_fn, base_seed, partition_method, activation_checkpoint_interval, activation_checkpoint_func, checkpointable_layers)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     raise RuntimeError(\n\u001b[0;32m--> 170\u001b[0;31m                         \u001b[0;34mf'num_stages ({self.num_stages}) must divide distributed world size ({self.world_size})'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                     )\n\u001b[1;32m    172\u001b[0m                 \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: num_stages (2) must divide distributed world size (1)"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(256, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "from deepspeed.pipe import PipelineModule\n",
    "net = PipelineModule(layers=net, num_stages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ba65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc47f2c65194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_world_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_group_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \"\"\"\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mGroupMember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORLD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mdefault_pg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_pg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         raise RuntimeError(\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;34m\"Default process group has not been initialized, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"please make sure to call init_process_group.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.distributed.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54429c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributed.init_process_group(backend=\"nccl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d636f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b53fc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_config = 'deepspeed_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97135d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.deepspeed_config = deepspeed_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547095c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b972c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-09 09:30:08,651] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.2, git-hash=unknown, git-branch=unknown\n",
      "[2022-09-09 09:30:08,653] [WARNING] [config.py:55:read_zero_config_deprecated] DeepSpeedConfig: this format of ZeRO optimization setup is deprecated. Please use the following format: \n",
      "ZeRO optimization should be enabled as:\n",
      "\"session_params\": {\n",
      "  \"zero_optimization\": {\n",
      "    \"stage\": [0|1|2],\n",
      "    \"stage3_max_live_parameters\" : 1000000000,\n",
      "    \"stage3_max_reuse_distance\" : 1000000000,\n",
      "    \"allgather_partitions\": [true|false],\n",
      "    \"allgather_bucket_size\": 500000000,\n",
      "    \"reduce_scatter\": [true|false],\n",
      "    \"contiguous_gradients\" : [true|false]\n",
      "    \"overlap_comm\": [true|false],\n",
      "    \"reduce_bucket_size\": 500000000,\n",
      "    \"load_from_fp32_weights\": [true|false],\n",
      "    \"cpu_offload\": [true|false] (deprecated),\n",
      "    \"cpu_offload_params\" : [true|false] (deprecated),\n",
      "    \"cpu_offload_use_pin_memory\": [true|false] (deprecated),\n",
      "    \"sub_group_size\" : 1000000000000,\n",
      "    \"offload_param\": {...},\n",
      "    \"offload_optimizer\": {...},\n",
      "    \"ignore_unused_parameters\": [true|false],\n",
      "    \"round_robin_gradients\": [true|false]\n",
      "    }\n",
      "}\n",
      "\n",
      "[2022-09-09 09:30:08,833] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/tmp/torch_extensions/fused_adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-28fe4708a104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_engine, optimizer, _, _ = deepspeed.initialize(args,\n\u001b[1;32m      2\u001b[0m                                                      \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                                      model_parameters=net.parameters())\n\u001b[0m",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/__init__.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_params)\u001b[0m\n\u001b[1;32m    132\u001b[0m                                  \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                                  \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                                  config_params=config_params)\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmpu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mpu must be None with pipeline parallelism\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_params, dont_change_device)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_parameters\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36m_configure_optimizer\u001b[0;34m(self, client_optimizer, model_parameters)\u001b[0m\n\u001b[1;32m   1116\u001b[0m                 \u001b[0mlog_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using client callable to create basic optimizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m             \u001b[0mbasic_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_basic_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m             log_dist(\n\u001b[1;32m   1120\u001b[0m                 \u001b[0;34mf\"Using DeepSpeed Optimizer param name {self.optimizer_name()} as basic optimizer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36m_configure_basic_optimizer\u001b[0;34m(self, model_parameters)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                         \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                         \u001b[0;34m**\u001b[0m\u001b[0moptimizer_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                         \u001b[0madam_w_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffective_adam_w_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m                     )\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/ops/adam/fused_adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, bias_correction, betas, eps, adam_w_mode, weight_decay, amsgrad, set_grad_none)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_grad_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mfused_adam_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFusedAdamBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dummy_overflow_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/ops/op_builder/builder.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjit_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/deepspeed/ops/op_builder/builder.py\u001b[0m in \u001b[0;36mjit_load\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    493\u001b[0m                            DEFAULT_TORCH_EXTENSION_PATH),\n\u001b[1;32m    494\u001b[0m             self.name)\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mstart_build\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/tmp/torch_extensions/fused_adam'"
     ]
    }
   ],
   "source": [
    "model_engine, optimizer, _, _ = deepspeed.initialize(args,\n",
    "                                                     model=net,\n",
    "                                                     model_parameters=net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "970480f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_config = NeoXArgsDeepspeedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3c147d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeoXArgsDeepspeedConfig:\n",
    "    \"\"\"\n",
    "    Args for deepspeed config\n",
    "    Every argument included here will be included in deepspeed config json\n",
    "    #TODO this list is not complete as compared to https://www.deepspeed.ai/docs/config-json/\n",
    "    \"\"\"\n",
    "\n",
    "    deepspeed: bool = True\n",
    "    \"\"\"boolean flag to enable DeepSpeed (Always True)\"\"\"\n",
    "\n",
    "    train_batch_size: int = None\n",
    "    \"\"\"\n",
    "    The effective training batch size. This is the amount of data samples that leads to one step of model update. train_batch_size is aggregated by the batch size that a single GPU processes in one forward/backward pass (a.k.a., train_step_batch_size), the gradient accumulation steps (a.k.a., gradient_accumulation_steps), and the number of GPUs.\n",
    "    \"\"\"\n",
    "\n",
    "    train_micro_batch_size_per_gpu: int = None\n",
    "    \"\"\"\n",
    "    Batch size to be processed by one GPU in one step (without gradient accumulation). When specified, gradient_accumulation_steps is automatically calculated using train_batch_size and number of GPUs. Should not be concurrently specified with gradient_accumulation_steps in the configuration JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    \"\"\"\n",
    "    Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. When specified, train_step_batch_size is automatically calculated using train_batch_size and number of GPUs. Should not be concurrently specified with train_step_batch_size in the configuration JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer: dict = None\n",
    "    \"\"\"\n",
    "    dict containing the keys type and params\n",
    "\n",
    "    type: The optimizer name. DeepSpeed natively supports Adam, AdamW, OneBitAdam, Lamb, and OneBitLamb optimizers (See here for details) and will import other optimizers from torch.\n",
    "\n",
    "    params: Dictionary of parameters to instantiate optimizer. The parameter names must match the optimizer constructor signature (e.g., for Adam).\n",
    "    \"\"\"\n",
    "\n",
    "    scheduler: dict = None\n",
    "    \"\"\"\n",
    "    dict containing the keys type and params\n",
    "\n",
    "    type: The scheduler name. See here (https://deepspeed.readthedocs.io/en/latest/schedulers.html) for list of support schedulers.\n",
    "\n",
    "    params: Dictionary of parameters to instantiate scheduler. The parameter names should match scheduler constructor signature.\n",
    "    \"\"\"\n",
    "\n",
    "    fp32_allreduce: bool = False\n",
    "    \"\"\"\n",
    "    During gradient averaging perform allreduce with 32 bit values\n",
    "    \"\"\"\n",
    "\n",
    "    prescale_gradients: bool = False\n",
    "    \"\"\"\n",
    "    Scale gradients before doing allreduce\n",
    "    \"\"\"\n",
    "\n",
    "    gradient_predivide_factor: float = 1.0\n",
    "    \"\"\"\n",
    "    Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs\n",
    "    \"\"\"\n",
    "\n",
    "    sparse_gradients: bool = False\n",
    "    \"\"\"\n",
    "    Enable sparse compression of torch.nn.Embedding gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    fp16: dict = None\n",
    "    \"\"\"\n",
    "    Configuration for using mixed precision/FP16 training that leverages NVIDIA’s Apex package.\n",
    "    \"\"\"\n",
    "\n",
    "    amp: dict = None\n",
    "    \"\"\"\n",
    "    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options\n",
    "    \"\"\"\n",
    "\n",
    "    gradient_clipping: float = 0.0\n",
    "    \"\"\"\n",
    "    Enable gradient clipping with provided value\n",
    "    \"\"\"\n",
    "\n",
    "    zero_optimization: dict = None\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    steps_per_print: int = 10\n",
    "    \"\"\"\n",
    "    Print train loss every N steps.\n",
    "    \"\"\"\n",
    "\n",
    "    wall_clock_breakdown: bool = False\n",
    "    \"\"\"\n",
    "    Enable timing of the latency of forward/backward/update training phases.\n",
    "    \"\"\"\n",
    "\n",
    "    dump_state: bool = False\n",
    "    \"\"\"\n",
    "    Print out state information of DeepSpeed object after initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    flops_profiler: dict = None\n",
    "    \"\"\"\n",
    "    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#flops-profiler\n",
    "    \"\"\"\n",
    "\n",
    "    zero_allow_untested_optimizer: bool = False\n",
    "    \"\"\"\n",
    "    Whether Deepspeed Zero Optimizer will allow an optimizer that hasn't been tested by the deepspeed team\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
